{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f2898b-2a95-4f28-870c-903b671a73fb",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Theory\n",
    "\n",
    "## Overfitting, Underfitting, and Generalized Model\n",
    "\n",
    "### Overfitting\n",
    "**Overfitting** happens when the model fits the training data too well. It performs well on the training data (low bias) but poorly on unseen test data (high variance), as it’s overly complex and not generalizable.\n",
    "\n",
    "In the graph below, we can see that the model fits well on training data (blue points) but does not fit well on test data (red points), as they are far from our predicted points (green points).\n",
    "\n",
    "![Overfitting Example](images/Overfitting_Example.png)  \n",
    "*Figure 1: Example of Overfitting, where the model fits the training data too closely but fails on test data.*\n",
    "\n",
    "---\n",
    "\n",
    "### Underfitting\n",
    "**Underfitting** occurs when the model is too simple to capture the data’s patterns. It performs poorly on both the training data and test data. This typically happens when the model has high bias, making it unable to learn from the data.\n",
    "\n",
    "In the graph below, we can see that the model does not fit well on both training data (blue points) and test data (red points).\n",
    "\n",
    "![Underfitting Example](images/Underfitting_Example.png)  \n",
    "*Figure 2: Example of Underfitting, where the model fails to capture the underlying patterns in the data.*\n",
    "\n",
    "---\n",
    "\n",
    "### Generalized Model\n",
    "A **well-generalized model** finds the balance between overfitting and underfitting, performing well on both training and test data by capturing the core patterns of the data without overcomplicating the model.\n",
    "\n",
    "In the graph below, we can see that the model fits well on both training data (blue points) and test data (red points).\n",
    "\n",
    "![Generalized Model Example](images/Generalized_Example.png)  \n",
    "*Figure 3: Example of a Generalized Model, demonstrating good performance on both training and test data.*\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "Achieving a balance between overfitting and underfitting is crucial for building robust predictive models. Techniques like Ridge and Lasso regression can help in this regard by adding regularization to the model training process, effectively controlling complexity and enhancing generalization.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Ridge And Lasso\n",
    "\n",
    "Let us take overfitting into consideration. In as overfitting model, the model fits the training data too well, often resulting in the cost function being very close to zero. Mathematically, we can describe the cost function as:  \n",
    "> J($θ_0$, $θ_1$) = $ \\frac{1}{2m} $ $ \\sum_{i=1}^{m} $ $ (h_θ(x_i) - y_i )^2 $\n",
    "\n",
    "\n",
    "When the cost function is 0, it means the difference between the predicted points and actual data points is zero, i.e., $ (h_θ(x_i) - y_i )^2 $ = 0. The model is fitting the data perfectly, which is not desirable as it does not generalize well to unseen data.\n",
    "\n",
    "Below is an example of an overfitting model, where the cost function is J(θ) = 0:  \n",
    "![Overfitting Example](images/Overfitting_Example2.png)  \n",
    "*Figure 4: Example of Overfitting, where the model fits the training data too closely.*\n",
    "\n",
    "Since overfitting is not an ideal model and we do not want the cost function to be 0, we use **Ridge and Lasso regularization.**\n",
    "\n",
    "---\n",
    "\n",
    "### Ridge (L2 Regularization)\n",
    "\n",
    "**Ridge regularization** adds an extra term to the cost function, preventing the model from fitting the data too closely. In this method, we add λ(slope)$^2$ to the cost function, where λ is a regularization parameter. The new cost function becomes:  \n",
    "> J($θ_0$, $θ_1$) = $ \\frac{1}{2m} $ $ \\sum_{i=1}^{m} $ $ (h_θ(x_i) - y_i )^2 + $ λ(slope)$^2$\n",
    "\n",
    "Here, λ is used to determine how fast we want to lessen or deepen the steepness of the best-fit line. \n",
    "\n",
    "In overfitting condition where cost function is 0, when we have added $ λ(slope)$^2$ it is no longer 0.\n",
    "\n",
    "Let us assume our slope value is 2 and λ value is 1.  \n",
    "![Overfitting Example](images/Overfitting_Example3.png)  \n",
    "*Figure 5: Example of Overfitting, where the model fits the training data too closely with a best-fit line of slope 2.*\n",
    "\n",
    "In this case our cost function is:  \n",
    "> J(θ) = 0 + 1(2)^2 = 4\n",
    "\n",
    "Now, the algorithm tries to minimize the cost further by reducing the slope, leading to a less aggressive fit. This introduces a small difference between the predicted and actual data points, resulting in a generalized model.\n",
    "\n",
    "Now, let us assume the slope is reduced to 1.5, and the cost function becomes:\n",
    "\n",
    "> J(θ) = (small value) + 1(1.5)$^2$ ≈ 3\n",
    "\n",
    "![Overfitting Example](images/Overfitting_Example4.png)  \n",
    "*Figure 6: This shows a new line introduced with a reduced slope with the help of regression.*\n",
    "\n",
    "We continue adjusting the slope iteratively until the cost function reaches its minimum, ensuring the best fit for the data. This process ultimately results in a generalized model that balances accuracy and overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Lasso (L1 Regularization)\n",
    "\n",
    "**Lasso regulization** adds an extra term to the cost function, preventing the model from overfitting and also reduces the number of features. Here, we add λ |slope| to the cost function where λ is a regularization parameter which is is used to determine how fast we want to lessen or deepen the steepness of the best-fit line.  \n",
    "The new cost function can be written as:\n",
    "> J($θ_0$, $θ_1$) = $ \\frac{1}{2m} $ $ \\sum_{i=1}^{m} $ $ (h_θ(x_i) - y_i )^2 + $ λ |slope|\n",
    "\n",
    "In lasso regularization, the overfitting is prevented by adding a term to the cost function which does not allow J(θ) or cost to be 0. It also simplifies the model by reducing the impact of less important features, as many of their coefficients get very close to zero, and some even become exactly zero, allowing the model to focus only on the features that truly matter.  \n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Both **Ridge (L2)** and **Lasso (L1)** regularization are powerful techniques for preventing overfitting by adding penalty terms to the cost function. Ridge regularization works by penalizing large coefficients, which reduces the complexity of the model without eliminating any features. On the other hand, Lasso not only penalizes large coefficients but also performs feature selection by shrinking some coefficients to zero, effectively removing less important features.\n",
    "\n",
    "Together, these regularization methods help create more generalized models that perform better on unseen data by finding a balance between underfitting and overfitting. The choice between Ridge and Lasso depends on the specific needs of the model—whether we want to reduce the complexity or focus on feature selection. Often, a combination of both methods, called Elastic Net, is used to get the best of both worlds.l.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
