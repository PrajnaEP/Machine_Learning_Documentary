{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015c33f9-621b-4c03-bf1e-fcd49f46b9af",
   "metadata": {},
   "source": [
    "# Random Forest Algorithms\n",
    "\n",
    "## What is the main problem in Decision Tree?\n",
    "\n",
    "The main issue with a Decision Tree Classifier is overfitting. This happens when the tree becomes too complex, trying to capture every detail in the training data, including noise or irrelevant patterns. As a result, it struggles to perform well on new, unseen data. Additionally, decision trees are sensitive to small changes in the training data, which can cause the entire tree structure to change.\n",
    "\n",
    "There are many methods in which the above mentioned problem can be solved. One of the methods is by using ensemble techniques. Lets see how using ensemble technique like bagging can help us overcome overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest: Bagging with Decision Trees  \n",
    "\n",
    "Random Forest is a machine learning method that uses bagging (Bootstrap Aggregating) to combine multiple decision trees. It improves accuracy and prevents overfitting.  \n",
    "\n",
    "### How Random Forest Works:  \n",
    "1. **Random Sampling:**  \n",
    "   - Random Forest creates many small datasets by randomly selecting samples from the original data (with replacement). Each of these datasets is used to train a decision tree.  \n",
    "\n",
    "2. **Random Feature Selection:**  \n",
    "   - When splitting nodes in each tree, Random Forest randomly picks a few features to consider, instead of using all the features. This adds variety to the trees.  \n",
    "\n",
    "3. **Combining Predictions:**  \n",
    "   - Once all the trees are trained:  \n",
    "     - For classification, the final result is based on the most common class predicted by the trees (majority vote).  \n",
    "     - For regression, the final prediction is the average of all tree predictions.  \n",
    "\n",
    "### Why Random Forest is Better:  \n",
    "\n",
    "- **Prevents Overfitting:**\n",
    "   - By averaging results from many trees, Random Forest avoids relying too much on specific patterns in the training data.    \n",
    "\n",
    "- **Handles Noisy Data Well:**\n",
    "   - Combining multiple trees reduces the impact of errors or outliers.  \n",
    "\n",
    "- **Stable and Reliable:** \n",
    "   - The randomness in both data sampling and feature selection ensures that no single tree dominates, making the model more consistent and accurate.\n",
    " \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
