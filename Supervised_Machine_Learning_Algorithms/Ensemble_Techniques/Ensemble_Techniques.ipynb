{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a133ab1b-58c2-4a36-be82-d4b4036e03d5",
   "metadata": {},
   "source": [
    "# Ensemble Techniques\n",
    "\n",
    "Ensemble techniques in machine learning are methods that combine predictions from multiple models to improve overall performance, reduce overfitting, and make predictions that are more robust. The aim is to aggregate diverse models to yield better results than using a single model.\n",
    "\n",
    "## Types Of Ensemble Techniques\n",
    "\n",
    "There are many types of ensemble techniques and the most commonly used ensemble techniques in machine learning are:\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "\n",
    "Let's look into each technique in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## Bagging\n",
    "\n",
    "In bagging, many different machine learning models are trained independently on subsets of the training data and the aggregated outcome is taken into consideration. By training different models independently, the overfitting and variance is reduced.\n",
    "\n",
    "### Working:\n",
    "\n",
    "- Generate multiple random samples from the training dataset.\n",
    "- Train a separate model on each sample.\n",
    "- Combine predictions:\n",
    "  - For classification: Use majority voting.\n",
    "  - For regression: Use averaging.\n",
    "\n",
    "### Example:\n",
    "\n",
    "- Suppose you are predicting whether a customer will buy a product (yes/no).\n",
    "- Dataset: You have 1000 samples.\n",
    "- Process:\n",
    "  - Create 10 sets of data with 100 samples in each.\n",
    "  - Train 10 decision trees, one on each sample.\n",
    "  - Combine predictions using majority voting (classification) or averaging (regression).\n",
    "\n",
    "---\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting technique trains models sequentially and each model attempts to correct the errors of previous model and makes the outcome accurate.This technique reduces bias and improves individual weak models.\n",
    "\n",
    "### Working:\n",
    "\n",
    "- Train a weak model on the full training dataset.\n",
    "- Calculate the errors.\n",
    "- Assign higher weights to the misclassified samples, marking them more important in the next iteration.\n",
    "- Train the next weak model on the weighted dataset.\n",
    "- Repeat the process for a fixed number of iterations or until performance stops improving.\n",
    "- Combine the predictions of all models, usually using a weighted sum.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose you are predicting whether a loan application will be approved (yes/no).- \n",
    "\n",
    "- Dataset: You have 500 samples.\n",
    "- Process:\n",
    "  - Train a weak learner (e.g., a simple decision tree) on the dataset.\n",
    "  - Focus on the samples that were misclassified by increasing their weights.\n",
    "  - Train another weak learner on the updated dataset.\n",
    "  - Combine the results of all weak learners using a weighted sum or majority voting.\n",
    " \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
