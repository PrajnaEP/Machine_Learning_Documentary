{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362f5c22-792e-4599-8633-589542e6222c",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Linear Regression Vs Logistic Regression\n",
    "\n",
    "To understand why we use logistic regression over linear regression, let's take an example where we predict the chances of heart disease based on cholesterol levels.\n",
    "\n",
    "| Cholesterol Level | Has Heart Disease (1 = Yes, 0 = No) |\n",
    "|-------------------|------------------------------------|\n",
    "| 180               | 0                                  |\n",
    "| 170               | 0                                  |\n",
    "| 200               | 0                                  |\n",
    "| 220               | 1                                  |\n",
    "| 225               | 1                                  |\n",
    "| 245               | 1                                  |\n",
    "\n",
    "Let's take a look at how cholesterol levels relate to heart disease using linear regression. The graph below shows the relationship between cholesterol levels and whether someone has heart disease:  \n",
    "![Cholestrol Vs Heart Disease Using Linear Regression](images/Cholestrol_Vs_Heart_Disease_LR.png)\n",
    "\n",
    "The red line is the best-fit line that linear regression generates. The green dotted line represents a threshold where the predicted probability equals 0.5. This threshold helps us decide: cholesterol levels to the left of the green line (lower levels) are predicted to not result in heart disease (0), while levels to the right (higher levels) are more likely to lead to heart disease (1).\n",
    "\n",
    "Linear regression fits the original data well, and the predicted outcomes align accurately. However, if we introduce more data points, such as a cholesterol level of 300 and beyond for individuals with heart disease (denoted by 1), the best-fit line changes significantly.\n",
    "\n",
    "Let's include these new data points and observe the effect on the model:  \n",
    "![Cholestrol Vs Heart Disease Using Linear Regression](images/Cholestrol_Vs_Heart_Disease_New_Data_LR.png)\n",
    "\n",
    "From the above graph, we can see that by adding these new, more extreme data points, the threshold point shifts where the predicted probability of heart disease crosses 0.5. This observation leads us to conclude that using linear regression for classification tasks, especially in the presence of unbalanced data points, is not recommended. \n",
    "\n",
    "Let's see how we can introduce logistic regression to tackle this problem. Unlike linear regression, which predicts continuous outcomes, logistic regression is designed for binary classification tasks. It estimates the probability that a given input belongs to a particular category—in this case, the likelihood of having heart disease based on cholesterol levels.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical model primarily used for binary classification tasks, where the objective is to predict one of two possible outcomes, typically denoted as 0 (no) or 1 (yes). This model is particularly useful in scenarios such as determining whether a patient has a disease based on various medical measurements or predicting whether a customer will purchase a product based on their browsing behavior.\n",
    "\n",
    "## Sigmoid Function\n",
    "\n",
    "In logistic regression, instead of drawing a linear best-fit line, we \"squash\" the linear line at both ends, making the model more suitable for binary classification. This is achieved by using the sigmoid function. \n",
    "\n",
    "In the graph below, we can see that our model is given more accurate results with the introduction of a sigmoid function.\n",
    "![Cholestrol Vs Heart Disease Using Logistic Regression](images/Cholesterol_Vs_Heart_Disease_Logistic_Regression.png)\n",
    "\n",
    "The mathematical representation of the sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **z** represents the linear combination of the input features and their weights $(i.e., z = \\theta_0 + \\theta_1 x )$.\n",
    "- **e** is the Euler's value which is approximately 2.71828.\n",
    "\n",
    "In the above equation, we are dividing 1 by a number which is sightly more than 1 and hence the result is less than one.The sigmoid function ensures that the output is always a value between 0 and 1, making it suitable for probability estimation.\n",
    "\n",
    "By substituting the value of z, into the sigmoid function, we obtain the logistic regression hypothesis:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x)}}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $h_\\theta(x)$ is the predicted probability that the outcome is 1 (i.e., the event occurs).\n",
    "- $\\theta_0$ is the intercept.\n",
    "- $\\theta_1$ is the slope of the line.\n",
    "\n",
    "This formula allows us to map any real-valued input to a value between 0 and 1, which represents the predicted probability of the binary outcome.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Function in Linear Regression\n",
    "\n",
    "In linear regression, the cost function is defined as:\n",
    "\n",
    "$$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)^2 $$\n",
    "\n",
    "Where:  \n",
    "- $h_\\theta(x)$ is the predicted outcome at that particular point in our model, calculated using a linear function $(i.e., z = \\theta_0 + \\theta_1 x )$.\n",
    "- $y_i$ represents the actual observed value.\n",
    "- $m$ is the number of training examples.\n",
    "\n",
    "In the context of logistic regression, however, the model predicts probabilities using the sigmoid function:\n",
    "\n",
    "$$ h_\\theta(x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x)}} $$\n",
    "\n",
    "This predicted outcome $h_\\theta(x)$ is bounded between 0 and 1, making it suitable for binary classification tasks.\n",
    "\n",
    "### Limitations of Using the Linear Cost Function in Logistic Regression\n",
    "\n",
    "#### Let's Understand with a Simple Example\n",
    "\n",
    "Let’s say we have the following data:\n",
    "\n",
    "| Hours Studied | Pass (1) / Fail (0) |\n",
    "|---------------|---------------------|\n",
    "| 1 hour        | 0 (Fail)            |\n",
    "| 2 hours       | 0 (Fail)            |\n",
    "| 5 hours       | 1 (Pass)            |\n",
    "| 6 hours       | 1 (Pass)            |\n",
    "\n",
    "If we use linear regression and predict:\n",
    "\n",
    "- A student studying 1 hour might get a prediction of 0.2 (which means they're 20% likely to pass, but the actual outcome is 0, fail).\n",
    "- A student studying 6 hours might get a prediction of 1.1 (which is not possible since the probability can’t be greater than 1, but the actual outcome is 1, pass).\n",
    "\n",
    "If we use the same approach in logistic regression, let’s see what will be the result:\n",
    "\n",
    "- For the student studying 1 hour (actual fail, 0), if we predict 0.2, the squared error is (0 - 0.2)² = 0.04.\n",
    "- For the student studying 6 hours (actual pass, 1), if we predict 1.1, the squared error is (1 - 1.1)² = 0.01.\n",
    "\n",
    "But in binary classification, predicting 0.2 for someone who is actually a 0 (fail) isn't the same kind of mistake as predicting 1.1 for someone who should have a 1 (pass). MSE doesn’t capture the difference between these errors. It's symmetric and treats both errors equally.\n",
    "\n",
    "In binary classification, the model should penalize large errors differently:\n",
    "- Predicting 0.9 when the actual class is 0 (big error) should be heavily penalized.\n",
    "- Predicting 0.2 when the actual class is 0 (small error) is less bad.\n",
    "\n",
    "Mean squared error doesn’t handle this asymmetry. It treats both errors as similarly \"bad.\" This can lead to poor model performance because it doesn’t properly emphasize the difference between \"almost correct\" and \"way off.\"\n",
    "\n",
    "---\n",
    "\n",
    "## The Logistic Regression Cost Function\n",
    "\n",
    "To fix the limitations of the linear cost function for binary classification, we need a new approach. Instead of using the Mean Squared Error (MSE), which doesn't handle the asymmetry of classification errors well, logistic regression introduces a more suitable cost function called the log-loss or cross-entropy loss.\n",
    "\n",
    "### The Mathematical Representation of the Cost Function\n",
    "\n",
    "The log-loss cost function for logistic regression looks like this:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $h_\\theta(x^{(i)})$ is the predicted outcome.\n",
    "- $y^{(i)}$ is the actual value or outcome.\n",
    "- $m$ is the length of data\n",
    "\n",
    "### Breaking It Down\n",
    "\n",
    "The cost function treats cases where y = 1 and y = 0 separately, using logarithms to ensure the penalty for mistakes grows larger as the model becomes more confident in its incorrect prediction.\n",
    "\n",
    "#### Case 1: When y = 1\n",
    "- The cost is $-\\log(h_\\theta(x)) $.\n",
    "- If the model predicts a probability $h_\\theta(x)$ close to 1, the cost is small (close to 0).\n",
    "- If $h_\\theta(x)$ is far from 1 (e.g., 0.1), the cost becomes very large.\n",
    "\n",
    "#### Case 2: When y = 0\n",
    "- The cost is $-\\log(1 - h_\\theta(x))$.\n",
    "- If the model predicts a probability $h_\\theta(x)$ close to 0, the cost is small.\n",
    "- If $h_\\theta(x)$ is far from 0 (e.g., 0.9), the cost becomes very large.\n",
    "\n",
    "The log function is important because it penalizes large errors much more heavily:\n",
    "- If the true label is 1, but the model predicts 0.9 (very close but not perfect), the penalty is small.\n",
    "- However, if the model predicts 0.1 (far from correct), the penalty becomes extremely large.\n",
    "\n",
    "This ensures the model focuses on minimizing big mistakes, making it more suitable for binary classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Class Classification Extension\n",
    "\n",
    "Logistic regression can be extended to multi-class classification using techniques like softmax regression or one-vs-all strategy. The cost function for multi-class classification is still based on cross-entropy, but it considers multiple class probabilities.\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
