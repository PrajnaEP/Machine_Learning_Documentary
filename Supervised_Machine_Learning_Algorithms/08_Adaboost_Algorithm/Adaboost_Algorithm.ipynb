{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb570007-5553-4680-a93f-a104fcea3414",
   "metadata": {},
   "source": [
    "# Adaboost\n",
    "\n",
    "Adaboost (short for **Adaptive Boosting**) is a powerful machine learning technique that combines multiple weak classifiers (models slightly better than random guessing) into a strong classifier. It works in stages, adjusting the importance (or weight) of each data point in the dataset at every step. This helps the algorithm focus on challenging examples as it learns.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Step-by-Step Explanation with a Dataset\n",
    "\n",
    "Let’s understand Adaboost with an example.\n",
    "\n",
    "### Example Dataset\n",
    "\n",
    "Here’s a dataset with 4 features and 7 records. The Output column shows whether the record belongs to class **Yes** or **No**.\n",
    "\n",
    "| F1   | F2   | F3   | F4   | Output |\n",
    "| ---- | ---- | ---- | ---- | ------ |\n",
    "| 1    | 2    | 3    | 4    | Yes    |\n",
    "| 2    | 3    | 1    | 4    | No     |\n",
    "| 3    | 4    | 2    | 1    | Yes    |\n",
    "| 4    | 1    | 4    | 3    | No     |\n",
    "| 2    | 1    | 3    | 4    | Yes    |\n",
    "| 3    | 3    | 4    | 2    | Yes    |\n",
    "| 1    | 4    | 2    | 3    | No     |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Initialize Weights\n",
    "\n",
    "Adaboost starts by giving **equal weights** to all records. Since there are 7 records, the weight for each is:\n",
    "\n",
    "$$\n",
    "\\text{Initial Weight} = \\frac{1}{\\text{Total Records}} = \\frac{1}{7} \\approx 0.1429\n",
    "$$\n",
    "\n",
    "| F1   | F2   | F3   | F4   | Output | Weight  |\n",
    "| ---- | ---- | ---- | ---- | ------ | ------- |\n",
    "| 1    | 2    | 3    | 4    | Yes    | 0.1429  |\n",
    "| 2    | 3    | 1    | 4    | No     | 0.1429  |\n",
    "| 3    | 4    | 2    | 1    | Yes    | 0.1429  |\n",
    "| 4    | 1    | 4    | 3    | No     | 0.1429  |\n",
    "| 2    | 1    | 3    | 4    | Yes    | 0.1429  |\n",
    "| 3    | 3    | 4    | 2    | Yes    | 0.1429  |\n",
    "| 1    | 4    | 2    | 3    | No     | 0.1429  |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Train a Weak Classifier (Stump)\n",
    "\n",
    "Adaboost uses a **weak learner**, such as a decision stump (a shallow decision tree), to split the data using one feature. For example, let’s say **F1** is chosen as the best feature, and a decision stump is trained. \n",
    "\n",
    "The stump predicts labels for all records. Assume it misclassifies **record 4**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Calculate Error and Update Weights\n",
    "\n",
    "#### 3.1 Total Error (TE)\n",
    "\n",
    "The **Total Error** of the weak classifier is the sum of the weights of misclassified records:\n",
    "\n",
    "$$\n",
    "TE = \\text{Sum of weights of misclassified records}\n",
    "$$\n",
    "\n",
    "In this case, only **record 4** is misclassified, so:\n",
    "\n",
    "$$\n",
    "TE = 0.1429\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2 Performance Score (PS)\n",
    "\n",
    "The performance score measures the effectiveness of the weak classifier and is calculated as:\n",
    "\n",
    "$$\n",
    "PS = \\frac{1}{2} \\log_e\\left(\\frac{1 - TE}{TE}\\right)\n",
    "$$\n",
    "\n",
    "Substituting \\( TE = 0.1429 \\):\n",
    "\n",
    "$$\n",
    "PS = \\frac{1}{2} \\log_e\\left(\\frac{1 - 0.1429}{0.1429}\\right) \\approx \\frac{1}{2} \\log_e(6) \\approx 0.895\n",
    "$$\n",
    "\n",
    "This value shows the influence of this weak classifier on the final model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3 Update Weights\n",
    "\n",
    "Weights are updated depending on whether the record was classified correctly or not:\n",
    "\n",
    "- **Correctly classified records**:  \n",
    "  Reduce their weights:  \n",
    "  $$ \\text{New Weight} = \\text{Old Weight} \\times e^{-PS} $$\n",
    "\n",
    "- **Incorrectly classified records**:  \n",
    "  Increase their weights:  \n",
    "  $$ \\text{New Weight} = \\text{Old Weight} \\times e^{PS} $$\n",
    "\n",
    "For **record 4** (incorrectly classified):  \n",
    "$$\n",
    "\\text{New Weight} ≈ 0.1429 \\times e^{0.895} ≈ 0.349\n",
    "$$\n",
    "\n",
    "For all correctly classified records:  \n",
    "$$\n",
    "\\text{New Weight} ≈ 0.1429 \\times e^{-0.895} ≈ 0.079\n",
    "$$\n",
    "\n",
    "| F1   | F2   | F3   | F4   | Output | Updated Weight |\n",
    "| ---- | ---- | ---- | ---- | ------ | -------------- |\n",
    "| 1    | 2    | 3    | 4    | Yes    | 0.079          |\n",
    "| 2    | 3    | 1    | 4    | No     | 0.079          |\n",
    "| 3    | 4    | 2    | 1    | Yes    | 0.079          |\n",
    "| 4    | 1    | 4    | 3    | No     | 0.349          |\n",
    "| 2    | 1    | 3    | 4    | Yes    | 0.079          |\n",
    "| 3    | 3    | 4    | 2    | Yes    | 0.079          |\n",
    "| 1    | 4    | 2    | 3    | No     | 0.079          |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Normalize Weights\n",
    "\n",
    "Normalize the weights so their sum equals 1:\n",
    "\n",
    "$$\n",
    "\\text{Normalized Weight} = \\frac{\\text{Updated Weight}}{\\text{Sum of Updated Weights}}\n",
    "$$\n",
    "\n",
    "| F1   | F2   | F3   | F4   | Output | Normalized Weight |\n",
    "| ---- | ---- | ---- | ---- | ------ | ----------------- |\n",
    "| 1    | 2    | 3    | 4    | Yes    | 0.096             |\n",
    "| 2    | 3    | 1    | 4    | No     | 0.096             |\n",
    "| 3    | 4    | 2    | 1    | Yes    | 0.096             |\n",
    "| 4    | 1    | 4    | 3    | No     | 0.424             |\n",
    "| 2    | 1    | 3    | 4    | Yes    | 0.096             |\n",
    "| 3    | 3    | 4    | 2    | Yes    | 0.096             |\n",
    "| 1    | 4    | 2    | 3    | No     | 0.096             |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Resample Records for the Next Stump\n",
    "\n",
    "Using the normalized weights, the dataset is resampled for the next iteration. Records with higher weights (e.g., **record 4**) are more likely to be selected. \n",
    "\n",
    "A random number between 0 and 1 determines which records are chosen, based on their weight ranges.\n",
    "\n",
    "| F1   | F2   | F3   | F4   | Output | Weight Range  |\n",
    "| ---- | ---- | ---- | ---- | ------ | ------------- |\n",
    "| 1    | 2    | 3    | 4    | Yes    | [0.00–0.096]  |\n",
    "| 2    | 3    | 1    | 4    | No     | [0.096–0.192] |\n",
    "| 3    | 4    | 2    | 1    | Yes    | [0.192–0.288] |\n",
    "| 4    | 1    | 4    | 3    | No     | [0.288–0.712] |\n",
    "| 2    | 1    | 3    | 4    | Yes    | [0.712–0.808] |\n",
    "| 3    | 3    | 4    | 2    | Yes    | [0.808–0.904] |\n",
    "| 1    | 4    | 2    | 3    | No     | [0.904–1.000] |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Iterate and Combine Stumps\n",
    "\n",
    "Repeat steps 2–5 for several iterations or until the error becomes small. Each weak classifier's predictions are weighted by its **performance score (PS)**. The final prediction is made using a weighted majority vote. And in case of adaboost regressor, we take the aggregated output as final prediction.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
